{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "This is a notebook to explore properties of embeddings, mostly the possibility to find semantically similar words or analogies for natural language with mathematical properties. The embeddings must be already downloadad in _data/glove.6B/glove.6B.100d.txt_, an example of how to do that can be found at _sample.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    ans = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            ans[word] = vector\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n"
     ]
    }
   ],
   "source": [
    "path = 'data/glove.6B/glove.6B.100d.txt'\n",
    "embeddings = load_embeddings(path)\n",
    "print(embeddings['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    ans = 0.0\n",
    "    prod = np.dot(u,v)\n",
    "    ans = prod / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word_1, sim_1, word_2, dic):\n",
    "    word_1 = word_1.lower()\n",
    "    sim_1 = sim_1.lower()\n",
    "    word_2 = word_2.lower()\n",
    "    emb_word_1, emb_sim_1, emb_word_2 = dic[word_1], dic[sim_1], dic[word_2]\n",
    "    # target = emb_sim_1 - emb_word_1 + emb_word_2\n",
    "    curr = -999\n",
    "    # min_diff = 100000\n",
    "    ans = 'hello'\n",
    "    for word, emb in dic.items():\n",
    "        if word in [word_1, sim_1, word_2]:\n",
    "            continue\n",
    "        # diff = np.linalg.norm(target - emb)\n",
    "        similarity = cosine_similarity(emb_sim_1 - emb_word_1, emb - emb_word_2)\n",
    "        if (similarity > curr):\n",
    "            curr = similarity\n",
    "            ans = word\n",
    "        # if (diff < min_diff):\n",
    "        #     ans = word\n",
    "        #     min_diff = diff\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man','king','woman',embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('austria','red','argentina',embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('austria','vienna','france',embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_similar(word_sim, dic, n=10):\n",
    "    word_emb = dic[word_sim]\n",
    "    # store (-distance, word) because it is a min heap\n",
    "    heap = []\n",
    "    for word, emb in dic.items():\n",
    "        if(word == word_sim):\n",
    "            continue\n",
    "        dist = np.linalg.norm(word_emb - emb)\n",
    "        if (len(heap) < n):\n",
    "            hp.heappush(heap,(-dist,word))\n",
    "        elif ((-heap[0][0])>dist):\n",
    "            #remove largest element and add current\n",
    "            hp.heappop(heap)\n",
    "            hp.heappush(heap,(-dist,word))\n",
    "    return [tup[1] for tup in heap] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slovenia',\n",
       " 'greece',\n",
       " 'poland',\n",
       " 'finland',\n",
       " 'lithuania',\n",
       " 'norway',\n",
       " 'bulgaria',\n",
       " 'belgium',\n",
       " 'sweden',\n",
       " 'latvia',\n",
       " 'slovakia',\n",
       " 'netherlands',\n",
       " 'romania',\n",
       " 'italy',\n",
       " 'hungary',\n",
       " 'denmark',\n",
       " 'germany',\n",
       " 'switzerland',\n",
       " 'austrian',\n",
       " 'luxembourg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_n_similar('austria',embeddings,n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
