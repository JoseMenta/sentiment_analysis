{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "This is a jupyter notebook for a project to detect the sentiments of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.951477Z",
     "start_time": "2024-09-26T13:13:46.590395Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4d6109c193390387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.966515Z",
     "start_time": "2024-09-26T13:13:57.956600Z"
    }
   },
   "source": [
    "print('hello world!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "375ea8f669085f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.980682Z",
     "start_time": "2024-09-26T13:13:57.971633Z"
    }
   },
   "source": [
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f0ca92c4d8e52647",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "We will use a dataset of movie reviews from IMDB, provided by [stanford university](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz).\n",
    "Then, we use the get_file function from keras (included in tensorflow) to download the dataset if it is not already in the cache_dir."
   ]
  },
  {
   "cell_type": "code",
   "id": "131913021967422f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.446923Z",
     "start_time": "2024-09-26T13:13:57.987932Z"
    }
   },
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                  untar=True, cache_dir='data',\n",
    "                                  cache_subdir='')"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\Itba2023\\2Q\\sentiment_analysis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:116\u001B[0m, in \u001B[0;36mextract_archive\u001B[1;34m(file_path, path, archive_format)\u001B[0m\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m         \u001B[38;5;66;03m# Tar archive, perhaps unsafe. Filter paths.\u001B[39;00m\n\u001B[1;32m--> 116\u001B[0m         \u001B[43marchive\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextractall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmembers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilter_safe_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43marchive\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (tarfile\u001B[38;5;241m.\u001B[39mTarError, \u001B[38;5;167;01mRuntimeError\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tarfile.py:2059\u001B[0m, in \u001B[0;36mTarFile.extractall\u001B[1;34m(self, path, members, numeric_owner)\u001B[0m\n\u001B[0;32m   2058\u001B[0m     \u001B[38;5;66;03m# Do not set_attrs directories, as we will do that further down\u001B[39;00m\n\u001B[1;32m-> 2059\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_attrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2060\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mnumeric_owner\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_owner\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2062\u001B[0m \u001B[38;5;66;03m# Reverse sort directories.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tarfile.py:2100\u001B[0m, in \u001B[0;36mTarFile.extract\u001B[1;34m(self, member, path, set_attrs, numeric_owner)\u001B[0m\n\u001B[0;32m   2099\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2100\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extract_member\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2101\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mset_attrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mset_attrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2102\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mnumeric_owner\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_owner\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2103\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tarfile.py:2173\u001B[0m, in \u001B[0;36mTarFile._extract_member\u001B[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001B[0m\n\u001B[0;32m   2172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tarinfo\u001B[38;5;241m.\u001B[39misreg():\n\u001B[1;32m-> 2173\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmakefile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargetpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2174\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m tarinfo\u001B[38;5;241m.\u001B[39misdir():\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tarfile.py:2214\u001B[0m, in \u001B[0;36mTarFile.makefile\u001B[1;34m(self, tarinfo, targetpath)\u001B[0m\n\u001B[0;32m   2213\u001B[0m bufsize \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopybufsize\n\u001B[1;32m-> 2214\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbltn_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtargetpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   2215\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maclImdb_v1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43muntar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mcache_subdir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Itba2023\\2Q\\sentiment_analysis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:315\u001B[0m, in \u001B[0;36mget_file\u001B[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001B[0m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m untar:\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(untar_fpath):\n\u001B[1;32m--> 315\u001B[0m         status \u001B[38;5;241m=\u001B[39m \u001B[43mextract_archive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatadir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marchive_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtar\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    316\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m status:\n\u001B[0;32m    317\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not extract archive.\u001B[39m\u001B[38;5;124m\"\u001B[39m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Itba2023\\2Q\\sentiment_analysis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:124\u001B[0m, in \u001B[0;36mextract_archive\u001B[1;34m(file_path, path, archive_format)\u001B[0m\n\u001B[0;32m    122\u001B[0m                 os\u001B[38;5;241m.\u001B[39mremove(path)\n\u001B[0;32m    123\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 124\u001B[0m                 \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrmtree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:759\u001B[0m, in \u001B[0;36mrmtree\u001B[1;34m(path, ignore_errors, onerror, dir_fd)\u001B[0m\n\u001B[0;32m    757\u001B[0m     \u001B[38;5;66;03m# can't continue even if onerror hook returns\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 759\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    615\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mislink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n\u001B[0;32m    616\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 617\u001B[0m     \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    615\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mislink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n\u001B[0;32m    616\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 617\u001B[0m     \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:617\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    615\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mislink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n\u001B[0;32m    616\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 617\u001B[0m     \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:620\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 620\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(fullname)\n\u001B[0;32m    621\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m    622\u001B[0m         onerror(os\u001B[38;5;241m.\u001B[39munlink, fullname, sys\u001B[38;5;241m.\u001B[39mexc_info())\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "3256dece9b514514",
   "metadata": {},
   "source": [
    "We define variables for the directories where the data is stored. Directories contain a text file for each review, positive ones are in the \n",
    "_pos_ directory and negative are in the _neg_ directory."
   ]
  },
  {
   "cell_type": "code",
   "id": "18170b4e61a89115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.469881Z",
     "start_time": "2024-09-26T13:16:00.464425Z"
    }
   },
   "source": [
    "dataset_dir=os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "train=os.path.join(dataset_dir,'train')\n",
    "test=os.path.join(dataset_dir,'test')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b958e138e36c82ef",
   "metadata": {},
   "source": [
    "We will remove the _unsup_ directory in the train data, as we will use supervised learning for this project (and this will simplify data\n",
    "loading in the following step)"
   ]
  },
  {
   "cell_type": "code",
   "id": "94211371c0869783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.475249Z",
     "start_time": "2024-09-26T13:16:00.474245Z"
    }
   },
   "source": [
    "remove=os.path.join(train,'unsup')\n",
    "shutil.rmtree(remove)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14abc96b709d8ae0",
   "metadata": {},
   "source": [
    "To load the data, we will use the _text_dataset_from_directory_ function in keras to load the data to memory. This function expect the \n",
    "structure \n",
    "```text\n",
    "dir/\n",
    "    class_1/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "    class_2/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "```\n",
    "which is conveniently followed by the dataset. In this case, neg will have the label 0 and pos the label 1. \n",
    "The dataset will be loaded in batches of 32 points (so it will yield groups of 32 points on each iteration) and the value 40 is used\n",
    "as a random seed for shuffling. It also will reserve 20% of the dataset for validation (used to tune hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdc1e4830e17184b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.480360Z",
     "start_time": "2024-09-26T13:16:00.477240Z"
    }
   },
   "source": [
    "batch_size = 32\n",
    "seed = 40\n",
    "\n",
    "raw_training_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74a49916b91b7571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.486786Z",
     "start_time": "2024-09-26T13:16:00.486786Z"
    }
   },
   "source": [
    "print(\"Label 0 corresponds to\", raw_training_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_training_ds.class_names[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a59143cd309656e2",
   "metadata": {},
   "source": [
    "raw_validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dc29fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.494042Z",
     "start_time": "2024-09-26T13:16:00.492039Z"
    }
   },
   "source": [
    "raw_testing_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test,\n",
    "    batch_size=batch_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6aa7a1fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "3d44e84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.496446Z",
     "start_time": "2024-09-26T13:16:00.495423Z"
    }
   },
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00a36629",
   "metadata": {},
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "#TODO: use glove embeddings\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization, #A function that is called for each input to standarize it\n",
    "    max_tokens=max_features, #maximum size of the vocabulary (it will have the \"top max_features\" words)\n",
    "    output_mode='int', #return an int for each word\n",
    "    output_sequence_length=sequence_length #TODO check!\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "649e7ddc",
   "metadata": {},
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_training_ds.map(lambda x, y: x)\n",
    "#build a vocabulary of all tokens in the dataset\n",
    "vectorize_layer.adapt(train_text) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fa8bb52",
   "metadata": {},
   "source": [
    "def vectorize_text(text, label):\n",
    "  print('previous expansion', vectorize_layer(text))\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b46602f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.507447Z",
     "start_time": "2024-09-26T13:16:00.507447Z"
    }
   },
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_training_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_training_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f06cdf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.511606Z",
     "start_time": "2024-09-26T13:16:00.511606Z"
    }
   },
   "source": [
    "train_ds = raw_training_ds.map(vectorize_text)\n",
    "val_ds = raw_validation_ds.map(vectorize_text)\n",
    "test_ds = raw_testing_ds.map(vectorize_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "autotune = tf.data.AUTOTUNE #dynamically change buffer size\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=autotune)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=autotune)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=autotune)"
   ],
   "id": "d1220c7f3ac2fb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "386b1602",
   "metadata": {},
   "source": [
    "# The neural network"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c49807f",
   "metadata": {},
   "source": [
    "embedding_dim = 16 \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim), #transform the word index into a vector of dimension embedding_dim (16)\n",
    "  layers.Dropout(0.2), # prevent overfitting\n",
    "  tf.keras.layers.LSTM(64),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])"
   ],
   "id": "ba633ec603319555",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ],
   "id": "eb3ec2f503c921ae"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.523197Z",
     "start_time": "2024-09-26T13:16:00.523197Z"
    }
   },
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ],
   "id": "65b233367933ff12",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ],
   "id": "cc853a696c3c53f8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ],
   "id": "f3eb4ac11de494c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
