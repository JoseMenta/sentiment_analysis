{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "This is a jupyter notebook for a project to detect the sentiments of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:28:14.753809Z",
     "start_time": "2024-09-18T22:26:57.337950Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 01:19:18.482279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6109c193390387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:38:20.640177Z",
     "start_time": "2024-09-18T22:38:20.635874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print('hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375ea8f669085f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:38:17.122631Z",
     "start_time": "2024-09-18T22:38:17.118128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca92c4d8e52647",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "We will use a dataset of movie reviews from IMDB, provided by [stanford university](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz).\n",
    "Then, we use the get_file function from keras (included in tensorflow) to download the dataset if it is not already in the cache_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131913021967422f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:45:28.967613Z",
     "start_time": "2024-09-18T22:43:38.076975Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                  untar=True, cache_dir='data',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256dece9b514514",
   "metadata": {},
   "source": [
    "We define variables for the directories where the data is stored. Directories contain a text file for each review, positive ones are in the \n",
    "_pos_ directory and negative are in the _neg_ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18170b4e61a89115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T23:00:16.017808Z",
     "start_time": "2024-09-18T23:00:16.013404Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir=os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "train=os.path.join(dataset_dir,'train')\n",
    "test=os.path.join(dataset_dir,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958e138e36c82ef",
   "metadata": {},
   "source": [
    "We will remove the _unsup_ directory in the train data, as we will use supervised learning for this project (and this will simplify data\n",
    "loading in the following step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94211371c0869783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:59:38.405926Z",
     "start_time": "2024-09-18T22:59:26.408292Z"
    }
   },
   "outputs": [],
   "source": [
    "remove=os.path.join(train,'unsup')\n",
    "shutil.rmtree(remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abc96b709d8ae0",
   "metadata": {},
   "source": [
    "To load the data, we will use the _text_dataset_from_directory_ function in keras to load the data to memory. This function expect the \n",
    "structure \n",
    "```text\n",
    "dir/\n",
    "    class_1/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "    class_2/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "```\n",
    "which is conveniently followed by the dataset. In this case, neg will have the label 0 and pos the label 1. \n",
    "The dataset will be loaded in batches of 32 points (so it will yield groups of 32 points on each iteration) and the value 40 is used\n",
    "as a random seed for shuffling. It also will reserve 20% of the dataset for validation (used to tune hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc1e4830e17184b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T23:07:21.893944Z",
     "start_time": "2024-09-18T23:07:19.820914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 40\n",
    "\n",
    "raw_training_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a49916b91b7571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T23:08:59.714789Z",
     "start_time": "2024-09-18T23:08:59.710273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59143cd309656e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_testing_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test,\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7a1fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d44e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a36629",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
