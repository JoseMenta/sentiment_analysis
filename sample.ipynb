{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "This is a jupyter notebook for a project to detect the sentiments of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.951477Z",
     "start_time": "2024-09-26T13:13:46.590395Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6109c193390387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.966515Z",
     "start_time": "2024-09-26T13:13:57.956600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print('hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375ea8f669085f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:13:57.980682Z",
     "start_time": "2024-09-26T13:13:57.971633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca92c4d8e52647",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "We will use a dataset of movie reviews from IMDB, provided by [stanford university](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz).\n",
    "Then, we use the get_file function from keras (included in tensorflow) to download the dataset if it is not already in the cache_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131913021967422f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.446923Z",
     "start_time": "2024-09-26T13:13:57.987932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                  untar=True, cache_dir='data',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256dece9b514514",
   "metadata": {},
   "source": [
    "We define variables for the directories where the data is stored. Directories contain a text file for each review, positive ones are in the \n",
    "_pos_ directory and negative are in the _neg_ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18170b4e61a89115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.469881Z",
     "start_time": "2024-09-26T13:16:00.464425Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir=os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "train=os.path.join(dataset_dir,'train')\n",
    "test=os.path.join(dataset_dir,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958e138e36c82ef",
   "metadata": {},
   "source": [
    "We will remove the _unsup_ directory in the train data, as we will use supervised learning for this project (and this will simplify data\n",
    "loading in the following step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94211371c0869783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.475249Z",
     "start_time": "2024-09-26T13:16:00.474245Z"
    }
   },
   "outputs": [],
   "source": [
    "remove=os.path.join(train,'unsup')\n",
    "shutil.rmtree(remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abc96b709d8ae0",
   "metadata": {},
   "source": [
    "To load the data, we will use the _text_dataset_from_directory_ function in keras to load the data to memory. This function expect the \n",
    "structure \n",
    "```text\n",
    "dir/\n",
    "    class_1/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "    class_2/\n",
    "        point_1.txt\n",
    "        point_2.txt\n",
    "```\n",
    "which is conveniently followed by the dataset. In this case, neg will have the label 0 and pos the label 1. \n",
    "The dataset will be loaded in batches of 32 points (so it will yield groups of 32 points on each iteration) and the value 40 is used\n",
    "as a random seed for shuffling. It also will reserve 20% of the dataset for validation (used to tune hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc1e4830e17184b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.480360Z",
     "start_time": "2024-09-26T13:16:00.477240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 40\n",
    "\n",
    "raw_training_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a49916b91b7571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.486786Z",
     "start_time": "2024-09-26T13:16:00.486786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_training_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_training_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59143cd309656e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc29fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.494042Z",
     "start_time": "2024-09-26T13:16:00.492039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_testing_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test,\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7a1fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d44e84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.496446Z",
     "start_time": "2024-09-26T13:16:00.495423Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a36629",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "#TODO: use glove embeddings\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization, #A function that is called for each input to standarize it\n",
    "    max_tokens=max_tokens, #maximum size of the vocabulary (it will have the \"top max_tokens\" words)\n",
    "    output_mode='int', #return an int for each word\n",
    "    output_sequence_length=sequence_length #TODO check!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649e7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 13:16:30.221881: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_training_ds.map(lambda x, y: x)\n",
    "#build a vocabulary of all tokens in the dataset\n",
    "vectorize_layer.adapt(train_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa8bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  print('previous expansion', vectorize_layer(text))\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b46602f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.507447Z",
     "start_time": "2024-09-26T13:16:00.507447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'I still liked it though. Warren Beatty is only fair as the comic book hero. What saves this movie is the set, the incredible cast and it offshoots a mediocre script. I really expected something more substantial in the terms of action, or plot but I got very little. The main reason to watch this movie is to watch some of the biggest stars in Hollywood at the time in such an unusual film. <br /><br />The one person who did a terrible job and did not even belong in this film was Madonna. She did not belong in this movie and her acting job was pretty bad. The movie at some points just stood still. You expected something more and you got nothing. Al Pacino plays a really bad dude and he does pretty good. He and Beatty do make an excellent good guy and bad guy. <br /><br />It is also interesting to see Dustin Hoffman, and Warren Beatty in a film other than Isthar. I did not see Ishtar but I heard bad things. The thing about this movie is it is good, but it could have been so much better. I liked it as a child because I thought it looked cool, and visually the movie is amazing, the sets are incredible, the writing is only fair, and with such a cast in the movie I would expect a little better anyway. <br /><br />SPOILER<br /><br />I especially thought the finale was not big enough. It was interesting but for such a grand scale film I just thought it could have ended with a little more of a bang. The reason to watch this movie is the atmosphere. The movie only uses the 7 comic book colors making it all that more excellent visually anyway.<br /><br />The costumes and makeup were flawless as well. The facial makeup for the villains was great. Beatty is just not up to such a bigger than life character. Still, this is a good movie that could have been much much more. It is to me better than Batman, the other comic book adaptation that came out close to the same time. Of course that movie was much bigger in terms of gross.', shape=(), dtype=string)\n",
      "Label pos\n",
      "previous expansion tf.Tensor(\n",
      "[  10  125  406    9  147 4041 4818    7   61 1214   14    2  734  271\n",
      "  657   48 2999   11   17    7    2  274    2 1007  173    4    9    1\n",
      "    3 1464  224   10   60  851  139   50 7346    8    2 1252    5  213\n",
      "   40  111   18   10  180   52  109    2  279  269    6  103   11   17\n",
      "    7    6  103   46    5    2 1085  384    8  353   30    2   59    8\n",
      "  136   33 1652   19    2   28  405   36  116    3  388  283    4  116\n",
      "   21   54 5110    8   11   19   13 4399   53  116   21 5110    8   11\n",
      "   17    4   39  112  283   13  178   83    2   17   30   46  749   41\n",
      " 3341  125   22  851  139   50    4   22  180  154 1496 2669  286    3\n",
      "   60   83 2545    4   27  121  178   49   27    4 4818   79   95   33\n",
      "  312   49  227    4   83  227    9    7   81  214    6   65 6813 2688\n",
      "    4 4041 4818    8    3   19   80   70    1   10  116   21   65    1\n",
      "   18   10  551   83  181    2  150   42   11   17    7    9    7   49\n",
      "   18    9   97   25   74   37   72  123   10  406    9   14    3  511\n",
      "   82   10  199    9  602  642    4 1881    2   17    7  477    2  709\n",
      "   23 1007    2  488    7   61 1214    4   16  136    3  173    8    2\n",
      "   17   10   58  517    3  109  123  554 1273   10  257  199    2 1917\n",
      "   13   21  195  189    9   13  214   18   15  136    3 1739], shape=(250,), dtype=int64)\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  10,  125,  406,    9,  147, 4041, 4818,    7,   61, 1214,   14,\n",
      "           2,  734,  271,  657,   48, 2999,   11,   17,    7,    2,  274,\n",
      "           2, 1007,  173,    4,    9,    1,    3, 1464,  224,   10,   60,\n",
      "         851,  139,   50, 7346,    8,    2, 1252,    5,  213,   40,  111,\n",
      "          18,   10,  180,   52,  109,    2,  279,  269,    6,  103,   11,\n",
      "          17,    7,    6,  103,   46,    5,    2, 1085,  384,    8,  353,\n",
      "          30,    2,   59,    8,  136,   33, 1652,   19,    2,   28,  405,\n",
      "          36,  116,    3,  388,  283,    4,  116,   21,   54, 5110,    8,\n",
      "          11,   19,   13, 4399,   53,  116,   21, 5110,    8,   11,   17,\n",
      "           4,   39,  112,  283,   13,  178,   83,    2,   17,   30,   46,\n",
      "         749,   41, 3341,  125,   22,  851,  139,   50,    4,   22,  180,\n",
      "         154, 1496, 2669,  286,    3,   60,   83, 2545,    4,   27,  121,\n",
      "         178,   49,   27,    4, 4818,   79,   95,   33,  312,   49,  227,\n",
      "           4,   83,  227,    9,    7,   81,  214,    6,   65, 6813, 2688,\n",
      "           4, 4041, 4818,    8,    3,   19,   80,   70,    1,   10,  116,\n",
      "          21,   65,    1,   18,   10,  551,   83,  181,    2,  150,   42,\n",
      "          11,   17,    7,    9,    7,   49,   18,    9,   97,   25,   74,\n",
      "          37,   72,  123,   10,  406,    9,   14,    3,  511,   82,   10,\n",
      "         199,    9,  602,  642,    4, 1881,    2,   17,    7,  477,    2,\n",
      "         709,   23, 1007,    2,  488,    7,   61, 1214,    4,   16,  136,\n",
      "           3,  173,    8,    2,   17,   10,   58,  517,    3,  109,  123,\n",
      "         554, 1273,   10,  257,  199,    2, 1917,   13,   21,  195,  189,\n",
      "           9,   13,  214,   18,   15,  136,    3, 1739]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_training_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_training_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cc074",
   "metadata": {},
   "source": [
    "Import embeddings from GloVe, with the 100d version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4f06159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    ans = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            ans[word] = vector\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af76aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26688    0.39632    0.6169    -0.77451   -0.1039     0.26697\n",
      "  0.2788     0.30992    0.0054685 -0.085256   0.73602   -0.098432\n",
      "  0.5479    -0.030305   0.33479    0.14094   -0.0070003  0.32569\n",
      "  0.22902    0.46557   -0.19531    0.37491   -0.7139    -0.51775\n",
      "  0.77039    1.0881    -0.66011   -0.16234    0.9119     0.21046\n",
      "  0.047494   1.0019     1.1133     0.70094   -0.08696    0.47571\n",
      "  0.1636    -0.44469    0.4469    -0.93817    0.013101   0.085964\n",
      " -0.67456    0.49662   -0.037827  -0.11038   -0.28612    0.074606\n",
      " -0.31527   -0.093774  -0.57069    0.66865    0.45307   -0.34154\n",
      " -0.7166    -0.75273    0.075212   0.57903   -0.1191    -0.11379\n",
      " -0.10026    0.71341   -1.1574    -0.74026    0.40452    0.18023\n",
      "  0.21449    0.37638    0.11239   -0.53639   -0.025092   0.31886\n",
      " -0.25013   -0.63283   -0.011843   1.377      0.86013    0.20476\n",
      " -0.36815   -0.68874    0.53512   -0.46556    0.27389    0.4118\n",
      " -0.854     -0.046288   0.11304   -0.27326    0.15636   -0.20334\n",
      "  0.53586    0.59784    0.60469    0.13735    0.42232   -0.61279\n",
      " -0.38486    0.35842   -0.48464    0.30728  ]\n"
     ]
    }
   ],
   "source": [
    "path = 'data/glove.6B/glove.6B.100d.txt'\n",
    "embeddings = load_embeddings(path)\n",
    "print(embeddings['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45025a56",
   "metadata": {},
   "source": [
    "Create a matrix to use in the embedding layer with the current vocabulary, where the i-th vector of the matrix is the embedding for the i-th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72db8d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 9888 words (112 misses)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "num_tokens = len(vocabulary) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits+=1\n",
    "    else:\n",
    "        misses+=1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f06cdf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.511606Z",
     "start_time": "2024-09-26T13:16:00.511606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous expansion Tensor(\"text_vectorization_1/Pad:0\", shape=(None, None), dtype=int64)\n",
      "previous expansion Tensor(\"text_vectorization_1/Pad:0\", shape=(None, None), dtype=int64)\n",
      "previous expansion Tensor(\"text_vectorization_1/Pad:0\", shape=(None, None), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_ds = raw_training_ds.map(vectorize_text)\n",
    "val_ds = raw_validation_ds.map(vectorize_text)\n",
    "test_ds = raw_testing_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1220c7f3ac2fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotune = tf.data.AUTOTUNE #dynamically change buffer size\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=autotune)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=autotune)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b1602",
   "metadata": {},
   "source": [
    "# The neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c49807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m1,000,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,200</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,200\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,200</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,200\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(num_tokens, embedding_dim, trainable=False,weights=[embedding_matrix]), #transform the word index into a vector of dimension embedding_dim (16)\n",
    "  layers.Dropout(0.2), # prevent overfitting\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba633ec603319555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ec2f503c921ae",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65b233367933ff12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:16:00.523197Z",
     "start_time": "2024-09-26T13:16:00.523197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 111ms/step - binary_accuracy: 0.5856 - loss: 0.7215 - val_binary_accuracy: 0.5648 - val_loss: 0.6836\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 114ms/step - binary_accuracy: 0.6059 - loss: 0.6549 - val_binary_accuracy: 0.7802 - val_loss: 0.4905\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 112ms/step - binary_accuracy: 0.7704 - loss: 0.5195 - val_binary_accuracy: 0.8150 - val_loss: 0.4253\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 109ms/step - binary_accuracy: 0.8083 - loss: 0.4488 - val_binary_accuracy: 0.8314 - val_loss: 0.4313\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 109ms/step - binary_accuracy: 0.7333 - loss: 0.6023 - val_binary_accuracy: 0.8240 - val_loss: 0.4105\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 91ms/step - binary_accuracy: 0.8013 - loss: 0.4565 - val_binary_accuracy: 0.8234 - val_loss: 0.4284\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 114ms/step - binary_accuracy: 0.8136 - loss: 0.4481 - val_binary_accuracy: 0.8286 - val_loss: 0.3924\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 112ms/step - binary_accuracy: 0.7942 - loss: 0.5035 - val_binary_accuracy: 0.8282 - val_loss: 0.4068\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 114ms/step - binary_accuracy: 0.8131 - loss: 0.4344 - val_binary_accuracy: 0.8306 - val_loss: 0.4020\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 96ms/step - binary_accuracy: 0.8239 - loss: 0.4234 - val_binary_accuracy: 0.7024 - val_loss: 0.5661\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc853a696c3c53f8",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3eb4ac11de494c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - binary_accuracy: 0.7069 - loss: 0.5630\n",
      "Loss:  0.5631871819496155\n",
      "Accuracy:  0.705079972743988\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
